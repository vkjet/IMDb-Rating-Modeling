---
title: "Modeling and prediction for movies"
author: Valeriy Kondruk
output: 
  html_document: 
    fig_height: 4
    highlight: pygments
    theme: spacelab
---

## Setup

### Load packages

```{r load-packages, message = FALSE}
library(ggplot2)
library(dplyr)
library(statsr)
library(GGally)
library(kableExtra) # create nice looking tables
```

### Load data

```{r load-data}
load("movies.Rdata")
```



* * *

## Part 1: Data

We have acquired data about how much audiences and critics like movies as well as numerous other variables about the movies. This dataset includes information from Rotten Tomatoes and IMDb for a random sample of movies.

We're interested in learning what attributes make a movie popular. 

The data set is comprised of 651 randomly sampled movies produced and released before 2016, and scored on both Rotten Tomatoes and IMDb.

We deal with a retrospective observational study with random sampling and with no random assignment here. Which means that we cannot make causal conclusions based on this data. However, we can use this data for making correlation statements, and the results of our findings can be generalized to the whole population of movies released before 2016 and scored on both Rotten Tomatoes and IMDb.

* * *

## Part 2: Research question

### Without watching a movie, can we say wether the audience love it?

Obviously, the popularity of a movie is mainly based on its content and the art value it offers. Those two factors are very hard if not impossible to calculate, though. However, there are certain characteristics of a movie that we can calculate and classify comparatively easy. Among others, those are genre, duration, and a cast quality. Using those and other variables from the data frame, we'll try creating a prediction model for a movie papularity among the audience.


* * *

## Part 3: Exploratory data analysis

We start our analysis with a high level overview of the data frame provided. The table below shows the number of movies of three major types (Documentary, Feature Film, and TV Movie) in the data set, the time range of theatre releases (`thtr_rel_year`), as well as the median audience rating on both IMDB (`imdb_rating`) and Rotten Tomatoes (`audience_score`). We choose median instead of mean for the audience rating as we suspect the rating distributions are skewed and median is a more robust statistics for skewed distributions.   

```{r}
movies %>%
  group_by(title_type) %>%
  summarise(n(), min(thtr_rel_year), max(thtr_rel_year), median(imdb_rating), median(audience_score)) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "condensed", "responsive"), full_width = F)
```
*Table 1. Movies statistics grouped by the tytle type*  
  
We can see the majority of movies in the data set belongs to the 'Feature Film' group. 55 movies are Documentaries and only 5 are TV Movies.  

It's expected that the popularity of movies in these three major groups would depend on very different factors. This partially confirmed by the significant difference between median rating of Featured Films and two other groups. Thus, we decided to deal only with the most populous group of movies in this data set, Featured Films. We then create a subset (*ff* for Feature Film) of the data frame and use it from now: 

```{r}
ff <- movies %>%
  filter(title_type == 'Feature Film')
```

Let's take a look at the data summary now (we exclude some variables from the summary to make the report shorter):  

```{r}
summary(ff[3:24])
```
  
The variables of most interest are related to movies rating: audience rating on IMDB (`imdb_rating`), critics score on Rotten Tomatoes (`critics_score`), and audience score on Rotten Tomatoes (`audience_score`). We can see that median statistics for all three variables are higher than respective mean statistics. This proves our early assumption of the skeweness of the rating distributions. In fact, all three distributions are **left skewed**. This can also be demonstrated with the following histograms:  
  
```{r}
hist(ff$imdb_rating, main = 'Plot 1. IMDB rating distribution', col = 'orange')
hist(ff$critics_score, main = 'Plot 2. Rotten Tomatoes critics score distribution', col = 'orange')
hist(ff$audience_score, main = 'Plot 3. Rotten Tomatoes audience score distribution', col = 'orange')
```
  
Interesting that IMDB rating shows a unimodal left skewed distribution centered around 6.4 (Plot 1) but both scores on Rotten Tomatoes show almost a uniform distribution with a slight left skew and no apparent centers (Plot 2 and 3). This discrepancy could be explained by the larger number of voting users on IMDb so the true population mean is more obvious. But this is questionable as we don't have data on the number of votes for Rotten Tomatoes scores. 

We can also conclude that **IMDB users are less likely to give a low rating to a movie** than Rotten Tomatoes users and critics. Indeed, 75% of movies received a rating of 58.5 (or 5.85 on a scale of 1 to 10) or higher on IMDB, 44.5 or higher from the audience on RT, and only 31 or higher from the critics on RT. 

However, the most plausible cause of the diferencies in distributions lies in methods of calculating rating and scores on two platforms:    
  
* IMDb registered users can cast a vote (from 1 to 10) on every released title in the database. Individual votes are then aggregated and summarized as a single IMDb rating, visible on the titleâ€™s main page. 
* Rotten Tomatoes critics score represents the percentage of professional critic reviews that are positive for a given film.
* Rotten Tomatoes audience score is the percentage of users who have rated the movie positively (a star rating of 3.5 or higher out of 5 stars). 
  
Basically, **the Rotten Tomatoes score only counts positive rates while IMDb rating counts all rates**. 

For example, an audience score of 25% on Rotten Tomatoes can be translated to an IMDb rating between 2.5 and 7.5 depending on individual ratings that made up that 25% score on RT. In other words, in the plots above we do not compare apples to apples (or we should say tomatoes to tomatoes).
  
Let's take a look at the median statistics for feature films grouped by genre:  

```{r}
ff %>%
  group_by(genre) %>%
  summarise(total = n(), IMDb = median(imdb_rating), RT_critics = median(critics_score), RT_audience = median(audience_score)) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "condensed", "responsive"))
```
*Table 2. Feature Films rating and scores medians by genre*

We can see that Drama is the most populous category with more than half movies of the data set and median IMDb rating of 6.80, RT critics score of 67, and RT audience score of 70. The highest median IMDb rating of 7.25 is in Musical & Performing Arts category. This group also has the highest scores of all on RT among both critics and audience. We need to note that only 8 movies fall in this group, so this stats can be quite different for a larger sample. The Comedy group has the lowest median IMDb rating of all, 5.70, second to lowest for RT critics and third to lowest for RT audience.
  
It's interesting to see the distribution of movies runtime:

```{r}
hist(ff$runtime, main = 'Plot 4. Feature Films runtime distribution', xlab = 'Runtime in minutes', col = 'orange')
```
  
Here we can see a right skewed unimodal distribution centered around 100 minutes with several outliers.  


* * *

## Part 4: Modeling

The data set contains several variables on movies: title, runtime, date of release, production company, cast, nominations, ratings, and scores. We'd love to predict the popularity of the movie (represented by the IMDb rating) based on a certain combination of the rest of the variables using a multiple linear regression method.

We will start with modelling where response variable is IMDb rating (`imdb_rating`).

### IMDb rating prediction model

Developing a full model as a reference to all future models would be a good first step. The full model includes all potential predictor variables. However, we would omit some of the variables as they are only in data set for informational purposes and do not make any sense to include in a statistical analysis. 
  
Here's a list of variables we can ommit:  

* The information in the `director`, and `actor1` through `actor5` variables was used to determine whether the movie casts a director, an actor or actress who won Oscar. 
* Variable like `imdb_url` and `rt_url` obviously cannot be associated with the popularity of a movie, thus should be ommited. 
* Wording of a `title` of the movie by itself is meaningless for answering the research question. But we can try using a title length as a predictor.
* We can omit the `title_type` variable as we only work with Feature Film here.
* The actual day of the month of the theatrical (`thtr_rel_day`) or dvd (`dvd_rel_day`) release is also meanengless for predicting movie's popularity. We can think of some correlation between a month or year and a movie popularity. We suspect it's going to be low if any, though. 
* We should also omit `critics_rating` and `audience_rating` variables as they are basically the derivatives of `critics_score` and `audience_score` variables respectively. So, they are obviously collinear and adding more than one of these variables to the model would not add much value to the model.
* We suspect a collinearity between `best_pic_nom` and `best_pic_win` variables. Obviously, movie can't win Oscar without being nominated. We should remove one of the variables, say, `best_pic_win` then.

The question is **whether we should use Rotten Tomatoes scores in predicting IMDb rating** and vice versa. Technically speaking, a popular movie would rate high on both platforms (it's not always the case, though). This means that rating on IMDb would have a positive correlation with audience score on RT despite the fact they are calculated differently. We can check if this is the case by calculating a correlation coefficient:

```{r}
cor(y = ff$imdb_rating, x = ff$audience_score)

ff %>%
  ggplot(aes(y = imdb_rating*10, x = audience_score)) +
  geom_jitter(col = 'orange') +
  labs(title = 'Plot 5. Correlation plot for IMDb rating and RT audience score') +
  geom_smooth(method = lm)

plot_ss(y = imdb_rating*10, x = audience_score, data = ff) +
  title(main = "Plot 6. Plot of residuals")

m_obs <- lm(imdb_rating ~ audience_score, data = ff)

ggplot(data = m_obs, aes(x = .fitted, y = .resid)) +
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Plot 7. Residuals vs fitted", xlab = "Fitted values", ylab = "Residuals")

ggplot(data = m_obs, aes(x = .resid)) +
  geom_histogram(binwidth = 0.5, fill = "orange", colour = "black") +
  xlab("Residuals") +
  ggtitle("Plot 8. Residulas distribution")

ggplot(data = m_obs, aes(sample = .resid)) +
  stat_qq(col = "orange") +
  ggtitle("Plot 9. Normal probability plot of the residuals") +
  geom_abline(colour = "blue")

```

Indeed, we can see that the correlation coefficient is very high (~0.85) and positive. However, the variance doesn't seem to be constant (little variance for popular movies and larger variance for unpopular movies) for the reason discussed in Part 3. There's definitely a correlation between these two variables but it's not linear. 

The previous assumption is even more apparent with RT critics score and critics rating variables plotted against IMDb audience rating (Plots 10-13):  

```{r}
ff %>%
  ggplot(aes(y = imdb_rating*10, x = critics_score)) +
  geom_jitter(col = 'orange', alpha = 0.7) +
  labs(title = 'Plot 10. Correlation plot for IMDb rating and RT critics score') +
  geom_smooth(method = lm)

m_obs3 <- lm(imdb_rating ~ critics_score, data = ff)

ggplot(data = m_obs3, aes(x = .fitted, y = .resid)) +
  geom_jitter(col = 'orange', alpha = 0.7) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Plot 11. Residuals vs fitted for imdb_rating/critics_score pair", x = "Fitted values", y = "Residuals")

ff %>%
  ggplot(aes(y = imdb_rating*10, x = critics_rating)) +
  geom_jitter(col = 'orange', alpha = 0.7) +
  labs(title = 'Plot 12. Correlation plot for IMDb rating and RT critics rating') +
  geom_smooth(method = lm)

m_obs4 <- lm(imdb_rating ~ critics_rating, data = ff)

ggplot(data = m_obs4, aes(x = .fitted, y = .resid)) +
  geom_jitter(col = 'orange', alpha = 0.7) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Plot 13. Residuals vs fitted for imdb_rating/critics_rating pair", x = "Fitted values", y = "Residuals")
```

The funnels we see is a perfect illustration of non-constant variability described earlier. Thus **we shouldn't be using RT scores in predicting IMDb rating using multiple linear regression**.  

The **full model** should then look like this:

```{r}
imdb_full <- lm(data = na.omit(ff), imdb_rating ~ genre + runtime + mpaa_rating + thtr_rel_year + thtr_rel_month + dvd_rel_year + dvd_rel_month + imdb_num_votes + best_pic_nom + best_actor_win + best_actress_win + best_dir_win + top200_box)

summary(imdb_full)
```

The full model includes all variables that we believe meaningful. It has an adjusted R^2^ of **`summary(imdb_full)$adj.r.squared`** which means that the model explains aproximately 36% of variance in the respose variable (`imdb_rating` in this case). Let's see if we can come up with a model of higher predicting power by changing a combination of predictors.    

We'll be using a **forward selection** technique here. The forward selection model starts with an empty model. Then, we add variables one-at-a-time until we cannot find any variables that improve the model (as measured by adjusted R^2^).  

```{r}
# Forward selection based on adjusted r squared

# Create a subset of data that only includes variables we're interested in

ff_imdb <- subset(na.omit(ff), select = c("imdb_rating", "genre", "runtime", "mpaa_rating", "thtr_rel_year", "thtr_rel_month", "dvd_rel_year", "dvd_rel_month", "imdb_num_votes", "best_pic_nom", "best_actor_win", "best_actress_win", "best_dir_win", "top200_box"))

bestformula <- "imdb_rating ~" #starting point

result <- 0
bestresult <- 0
cycle = 2 #start with 2nd column of a data frame

while (cycle < ncol(ff_imdb)) {
  i = 2 #start with 2nd column of a data frame
  while (i <= ncol(ff_imdb)) {
    midformula = paste(as.character(bestformula), colnames(ff_imdb)[i], sep = " + ")
    midmodel = lm(as.formula(midformula), data = ff_imdb)
    midres = summary(midmodel)$adj.r.squared
    
    if (midres > result) {
      result = midres
      resformula = midformula
      resmodel = midmodel
    }
    i = i + 1 
  }
  
  if (result > bestresult) {
    bestformula = resformula
    cycle = cycle + 1
  } else {
    break
  }
}

best_imdb_fwd_model = lm(bestformula, data = ff_imdb)
summary(best_imdb_fwd_model)
```

As a result, we come up with a model **`r bestformula`** with adjusted R^2^ of **`r summary(best_imdb_fwd_model)$adj.r.squared`** which is slightly higher than that of a full model (`r summary(imdb_full)$adj.r.squared`). However, we can see that some of the variables have  a p-value above the significance level of 5%. The order of certain variable in the formula tells us about its impact on the adjusted R^2^ (the earlier in the formula, the greater the impact).    


Let's try a backward elimination technique and compare the resulting model with the model above. **Backward elimination** starts with the model that includes all potential predictor variables. Variables are eliminated one-at-a-time from the model until we cannot improve the adjusted R^2^. At each step we eliminate the variable that leads to the largest improvement in adjusted R^2^.

```{r}
#predictors <- model.matrix(imdb_full)[,-1] # move the names of the predictors to matrix 
bwrd_bestpredictors = c("genre", "runtime", "mpaa_rating", "thtr_rel_year", "thtr_rel_month", "dvd_rel_year", "dvd_rel_month", "imdb_num_votes", "best_pic_nom", "best_actor_win", "best_actress_win", "best_dir_win", "top200_box")
midpredictors = bwrd_bestpredictors
bwrd_result <- summary(imdb_full)$adj.r.squared
test_predictors = c()

while (length(bwrd_bestpredictors) > 0) {
  i = 1 
  adjr2 = c()
  while (i <= length(midpredictors)) {
    test_predictors = midpredictors[-(i)]
    midformula = as.formula(paste("imdb_rating ~ ", paste(test_predictors, collapse = " + "), sep = ""))
    midmodel = lm(midformula, data = ff_imdb)
    midres = summary(midmodel)$adj.r.squared
    adjr2 = append(adjr2, midres, after = length(adjr2))
    i = i + 1 
    }

  if (max(adjr2) > bwrd_result) {
    midpredictors = midpredictors[-(which.max(adjr2))]
    bwrd_bestpredictors = midpredictors
    bwrd_result = max(adjr2)
  } else {break}
}

bwrd_bestformula = as.formula(paste("imdb_rating ~ ", paste(bwrd_bestpredictors, collapse = " + "), sep = ""))
imdb_bwrd_best_model = lm(bwrd_bestformula, data = ff_imdb)
summary(imdb_bwrd_best_model)

```

We ended up with **the same model** using backward elimination technique. We should run a diagnostic for the following predictors included in a model:

**`r bwrd_bestpredictors`**  
  

### Model diagnostics

To assess whether the multiple regression model is reliable, we need to check for:
  
1. nearly normal residuals,
2. constant variability of residuals,
4. the residuals are independent, and
4. each variable is linearly related to outcome.

Let's take a look ath the **Normal probability plot of residuals** for our model.  

```{r}
ggplot(data = best_imdb_fwd_model, aes(sample = .resid)) +
  stat_qq(col = "orange", alpha = 0.5) +
  ggtitle("Plot 14. Normal probability plot of the residuals for IMDb rating prediction model") +
  geom_abline(colour = "blue")
```

We can see some fluctuations from a normal model. They are not extreme, though. There are no outliers that might be cause of concern. In a normal probability plot for residuals, we tend to be most worried about residuals that appear to be outliers, since these indicate long tails in the distribution of residuals.

**Absolute values of residuals against fitted values.** The following plot is helpful to check the condition that the variance of the residuals is approximately constant.

```{r}
ggplot(data = best_imdb_fwd_model, aes(x = round(.fitted, 0), y = abs(.resid))) +
  geom_jitter(col = 'orange', alpha = 0.7) +
  labs(title = "Plot 15. Absolute values of residuals vs fitted for IMDb rating model", x = "Fitted values of IMDb rating (rounded to the nearest whole number)", y = "Absolute values of residuals")
```

We don't see any obvious deviations from constant variance in this plot. As there are not many movies with IMDb ratings around 8, 9, or 10 in the data set, we don't know if the variance for these values remains constant. However, there is no evedence that it doesn't based on the data we have.

**Residuals in order of their data collection.** Such a plot is helpful in identifying any connection between cases that are close to one another. If it seems that consecutive observations tend to be close to each other, this indicates the independence assumption of the observations would fail. We know that the data set represents a random selection of movies, so we don't expect any problems here.

```{r}
plot(best_imdb_fwd_model$residuals, main = "Plot 16. Residuals in order of their data collection", xlab = "Order", ylab = "Residuals", col = "orange")
```

As expected, here we see no structure that indicates a problem.

Last thing we need to check is **Residuals against each predictor variable.** The first row of the graphics below shows the residual plots:   

```{r fig.height=7, fig.width=14, message=FALSE, warning=FALSE, out.height=1200, out.width=2400}
#ggscatmat(ff_imdb, columns = c("imdb_rating", "imdb_num_votes", "runtime", "thtr_rel_year", "dvd_rel_month"), alpha = 0.5)

ggnostic(imdb_bwrd_best_model, mapping = ggplot2::aes(color = imdb_rating)) +
  labs(title = "Plot 17. Residuals of the model against each predictor variable")
```

Let's look at the **categorical variables** first. For the `genre` variable we see a lot of deviations from constant variability between different genres here. The variability among the groups in `mpaa_rating` seems a little bit more constant. For example, Unrated movies (there are only 16 of them in a data frame) have significantly less variability than others and NC-17 has no variability at all (in fact, there's only one movie of this category in a data set). Besides that, this variable look ok. Both `best_pic_nom`, `best_dir_win` variables show the inconstant variability (the latter in a lesser extent, though). On the one hand this can be explained by a significantly smaller number of movies or directors that have been nominated for Oscar. However, the reals reason is not clear from the data. 

The **numerical variables** show more uniformity overall. With theatrical release year (`thtr_rel_year`) and DVD release year (`dvd_rel_year`) we don't see a structure (distribution around 0 seems to be random). There might be some remaining structure in the DVD release month variable, though. We can see a little 'wave' going up and down here. There's a clear 'funneling' in both `runtime` and `imdb_num_votes` variables. For instance, in the IMDb number of votes, prediction for the movies with lower number of votes has lower accuracy (larger difference between predicted and observed value) than for those with large number of votes - this is not something unexpected as point estimate is getting closer to the actual population parameter as the sample size increases. It's harder to explain why the accuracy of the model goes down for the shorter movies, though.

It's worth mentioning that the following variables have the p-value above the significance level of 5%: `best_dir_win`, `best_pic_nom`, `dvd_rel_year`, and `dvd_rel_month`. 
  
  
### Interpretation of model coefficients

Table below shows the coefficients (or point estimates of the coefficients) for each predictor in our model. 

```{r}
summary(imdb_bwrd_best_model)$coeff %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), fixed_thead = T)
```

In this case Intercept is a point where regression line intercepts the y-axis represented by `imdb_rating`. The coefficient of 0.0055 for `runtime` variable means that the average difference in IMDb rating for each additional minute of a runtime is +0.0055 when **holding the other variables constant**.

We can see that some of the variables have larger impact on the IMDb rating. For example, if MPAA rating of a movie is PG-13 ("Parental Guidance: some material may be inappropriate for children 13 and under") reduces the IMDb rating of the movie by 0.8. A movie in Musical & Performing Arts category increses the rating by 1.19. It's a largest single factor.

The number of votes on IMDb (`imdb_num_votes`) seems to be having the least impact on the movie's IMDb rating: each additional vote adds up only 0.0000035 to the rating. A movie needs a million votes to see a rating increased by 3.5. Considering that a movie in our data set gets on average only 62,861 votes the impact on the rating is not very large (0.22 on average).

As expected, both the DVD release year and month have just a tiny impact on IMDb rating (if any, considering their p-values above the significance level). The Best picture nomination adds up 0.38 to the rating.


```{r eval=FALSE, , fig.show='hide', include=FALSE, results='hide'}
# This current chunk won't be included in the knit as the followng techniques are not part of the Duke's course.

# Best subsets is a technique that relies on stepwise regression to search, find and visualise regression models.

library(leaps)

subsets <- regsubsets(imdb_rating ~ ., ff_imdb, nvmax = 30, method = "backward")

plot(subsets, scale = "adjr2") +
  title("Plot 14. ")

#with(summary(subsets), data.frame(adjr2, outmat))
#best_model <- summary(subsets)$which[which((summary(subsets)$adjr2 == max(summary(subsets)$adjr2))),]

coef(subsets, which.max(summary(subsets)$adjr2))
max(summary(subsets)$adjr2)
summary(subsets)$adjr2

leaps_subsets <- leaps(x = predictors, y = na.omit(ff)$imdb_rating, method = "adjr2")
widx = which.max(leaps_subsets$adjr2)
xidx = (1:ncol(predictors))[leaps_subsets$which[widx,]]
Xin = data.frame(predictors[,xidx])
best_model <- lm(na.omit(ff)$imdb_rating ~ . , data=Xin)
summary(best_model)
summary(best_model)$adj.r.squared
```

```{r eval=FALSE, include=FALSE}
library(olsrr)

imdb_backward_p <- ols_step_backward_p(imdb_full, details = F, p = 0.05)
```




* * *

## Part 5: Prediction

To check our model's prediction accuracy we picked a movie that is not in the initial data set and that was released in 2016. This movie is **"La La Land"** (IMDb link: www.imdb.com/title/tt3783958/). As of December 17, 2019 it has an IMDb rating of 8.0 and 456,399 votes. The movie and its director, Damiene Chazelle, were both nominated and won Oscars in 2017. The rest of the parameters has been combined in a data frame `lalaland`.

We use a `predict` function and use both our model and the `lalaland` data as an input:

```{r}
lalaland <- data.frame(genre = "Comedy", runtime = 128, mpaa_rating = "PG-13", thtr_rel_year = 2016, dvd_rel_year = 2017, dvd_rel_month = 4, imdb_num_votes = 456399, best_pic_nom = "yes", best_dir_win = "yes")
lalaland_prediction <- predict(imdb_bwrd_best_model, lalaland, interval = "prediction", level = 0.95)
lalaland_prediction
```

The **model predicts an IMDb rating of 7.35** for this movie. The actual rating is 8.0. This means that model underestimated real rating. The `lwr` and `upr` values show lower and upper bounds of a confidence interval of the prediction with a confidence level of 95%. We're 95% confident that the actual IMDb rating of this movie is between 5.62 and 9.07. The interval is quite wide. This in fact agrees with the adjusted R^2^ value of 0.3644 meaning that only 36.44% of the variation in rating is explained by the model. 

We use a movie released in 2016 despite the fact that our model is based on data about movies released before 2016. This implyes an extrapolation which makes our prediction even less reliable. 

* * *

## Part 6: Conclusion

In this research, we used factual data on 600+ movies and built a multiple regression model that predicts IMDb rating. The Adjusted R^2^ was used as estimate of explained variance. We tried two techniques of stepwise model selection (forward selection and backward elimination) which came up with the same model that explains approximately 36.44% of variation in movie popularity measured by IMDb rating. 

Answering the research question, we can predict the popularity of a movie without watching it but to a certain limited extent.

Fortunately for us, movie lovers, the popularity of a certain piece of art is based on something that is hard to measure and categorize. And this 'something' explains a good part of those remaining 63.56%. 

